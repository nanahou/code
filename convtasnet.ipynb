{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convtasnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMv3JQbhvtHZw5+6ElnqtqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanahou/code/blob/master/convtasnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdqJJLTGMkrU"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import pprint\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "import glob\n",
        "import os.path\n",
        "import soundfile as sf\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7eTfaT7PT97"
      },
      "source": [
        "# process speech data\n",
        "# *extract features\n",
        "# define model\n",
        "# define dataloader\n",
        "# define train scripts\n",
        "# define test scripts\n",
        "# define configuration"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ICjGvXgQKJ3"
      },
      "source": [
        "# process speech data to same length 1s\n",
        "def splitTo1s(clean_dir, noisy_dir, cache_dir_d, cache_dir_c, slice_size, data_stride):\n",
        "\n",
        "  # slice_size = 16384\n",
        "  # data_stride = 0.5\n",
        "\n",
        "  clean_names = glob.glob(os.path.join(clean_dir, '*.wav')) #search .wav in clean_dir\n",
        "  noisy_names = glob.glob(os.path.join(noisy_dir, '*.wav'))\n",
        "  print('Found {} clean names and {} noisy names'.format(len(clean_names), len(noisy_names)))\n",
        "  if len(clean_names) != len(noisy_names) or len(clean_names) == 0:\n",
        "      raise ValueError('No wav data found! Check your data path please')\n",
        "\n",
        "  print('now we split ', clean_dir, 'wavs!')\n",
        "  for name in clean_names:\n",
        "      wav, rate = sf.read(name)\n",
        "      length = len(wav)\n",
        "      num_slice = int((length-slice_size) / (slice_size*data_stride) + 1)\n",
        "      start = 0\n",
        "      base_name = os.path.splitext(os.path.basename(name))[0]\n",
        "      for i in range(num_slice):\n",
        "          slice = wav[start:start+slice_size]\n",
        "          start += int(slice_size/2)\n",
        "          slice_name = os.path.join(os.path.dirname(cache_dir_c), base_name + '_' + str(i) + '.wav')\n",
        "          sf.write(slice_name, slice, rate)\n",
        "\n",
        "  print('now we split ', noisy_dir, 'wavs!')\n",
        "  for name in noisy_names:\n",
        "      wav, rate = load(name)\n",
        "      length = len(wav)\n",
        "      num_slice = int((length-slice_size) / (slice_size*data_stride) + 1)\n",
        "      start = 0\n",
        "      base_name = os.path.splitext(os.path.basename(name))[0]\n",
        "      for i in range(num_slice):\n",
        "          slice = wav[start:start+slice_size]\n",
        "          start += int(slice_size/2)\n",
        "          slice_name = os.path.join(os.path.dirname(cache_dir_d), base_name + '_' + str(i) + '.wav')\n",
        "          sf.write(slice_name, slice, rate)\n",
        "\n",
        "  clean_slice_names = glob.glob(os.path.join(cache_dir_c, '*.wav'))\n",
        "  noisy_slice_names = glob.glob(os.path.join(cache_dir_d, '*.wav'))\n",
        "  print('Found {} clean names and {} noisy names'.format(len(clean_slice_names), len(noisy_slice_names)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6aYc3ahTAsW",
        "outputId": "bf613073-cb34-4808-f5c1-ec113be039ab"
      },
      "source": [
        "# define models\n",
        "def param(nnet, Mb=True):\n",
        "    \"\"\"\n",
        "    Return number parameters(not bytes) in nnet\n",
        "    \"\"\"\n",
        "    neles = sum([param.nelement() for param in nnet.parameters()])\n",
        "    return neles / 10**6 if Mb else neles\n",
        "\n",
        "\n",
        "class ChannelWiseLayerNorm(nn.LayerNorm):\n",
        "    \"\"\"\n",
        "    Channel wise layer normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ChannelWiseLayerNorm, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: N x C x T\n",
        "        \"\"\"\n",
        "        if x.dim() != 3:\n",
        "            raise RuntimeError(\"{} accept 3D tensor as input\".format(\n",
        "                self.__name__))\n",
        "        # N x C x T => N x T x C\n",
        "        x = th.transpose(x, 1, 2)\n",
        "        # LN\n",
        "        x = super().forward(x)\n",
        "        # N x C x T => N x T x C\n",
        "        x = th.transpose(x, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GlobalChannelLayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Global channel layer normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, eps=1e-05, elementwise_affine=True):\n",
        "        super(GlobalChannelLayerNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.normalized_dim = dim\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if elementwise_affine:\n",
        "            self.beta = nn.Parameter(th.zeros(dim, 1))\n",
        "            self.gamma = nn.Parameter(th.ones(dim, 1))\n",
        "        else:\n",
        "            self.register_parameter(\"weight\", None)\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: N x C x T\n",
        "        \"\"\"\n",
        "        if x.dim() != 3:\n",
        "            raise RuntimeError(\"{} accept 3D tensor as input\".format(\n",
        "                self.__name__))\n",
        "        # N x 1 x 1\n",
        "        mean = th.mean(x, (1, 2), keepdim=True)\n",
        "        var = th.mean((x - mean)**2, (1, 2), keepdim=True)\n",
        "        # N x T x C\n",
        "        if self.elementwise_affine:\n",
        "            x = self.gamma * (x - mean) / th.sqrt(var + self.eps) + self.beta\n",
        "        else:\n",
        "            x = (x - mean) / th.sqrt(var + self.eps)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"{normalized_dim}, eps={eps}, \" \\\n",
        "            \"elementwise_affine={elementwise_affine}\".format(**self.__dict__)\n",
        "\n",
        "\n",
        "def build_norm(norm, dim):\n",
        "    \"\"\"\n",
        "    Build normalize layer\n",
        "    LN cost more memory than BN\n",
        "    \"\"\"\n",
        "    if norm not in [\"cLN\", \"gLN\", \"BN\"]:\n",
        "        raise RuntimeError(\"Unsupported normalize layer: {}\".format(norm))\n",
        "    if norm == \"cLN\":\n",
        "        return ChannelWiseLayerNorm(dim, elementwise_affine=True)\n",
        "    elif norm == \"BN\":\n",
        "        return nn.BatchNorm1d(dim)\n",
        "    else:\n",
        "        return GlobalChannelLayerNorm(dim, elementwise_affine=True)\n",
        "\n",
        "\n",
        "class Conv1D(nn.Conv1d):\n",
        "    \"\"\"\n",
        "    1D conv in ConvTasNet\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Conv1D, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x, squeeze=False):\n",
        "        \"\"\"\n",
        "        x: N x L or N x C x L\n",
        "        \"\"\"\n",
        "        if x.dim() not in [2, 3]:\n",
        "            raise RuntimeError(\"{} accept 2/3D tensor as input\".format(\n",
        "                self.__name__))\n",
        "        x = super().forward(x if x.dim() == 3 else th.unsqueeze(x, 1))\n",
        "        if squeeze:\n",
        "            x = th.squeeze(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTrans1D(nn.ConvTranspose1d):\n",
        "    \"\"\"\n",
        "    1D conv transpose in ConvTasNet\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ConvTrans1D, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x, squeeze=False):\n",
        "        \"\"\"\n",
        "        x: N x L or N x C x L\n",
        "        \"\"\"\n",
        "        if x.dim() not in [2, 3]:\n",
        "            raise RuntimeError(\"{} accept 2/3D tensor as input\".format(\n",
        "                self.__name__))\n",
        "        x = super().forward(x if x.dim() == 3 else th.unsqueeze(x, 1))\n",
        "        if squeeze:\n",
        "            x = th.squeeze(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Conv1DBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    1D convolutional block:\n",
        "        Conv1x1 - PReLU - Norm - DConv - PReLU - Norm - SConv\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels=256,\n",
        "                 conv_channels=512,\n",
        "                 kernel_size=3,\n",
        "                 dilation=1,\n",
        "                 norm=\"cLN\",\n",
        "                 causal=False):\n",
        "        super(Conv1DBlock, self).__init__()\n",
        "        # 1x1 conv\n",
        "        self.conv1x1 = Conv1D(in_channels, conv_channels, 1)\n",
        "        self.prelu1 = nn.PReLU()\n",
        "        self.lnorm1 = build_norm(norm, conv_channels)\n",
        "        dconv_pad = (dilation * (kernel_size - 1)) // 2 if not causal else (\n",
        "            dilation * (kernel_size - 1))\n",
        "        # depthwise conv\n",
        "        self.dconv = nn.Conv1d(\n",
        "            conv_channels,\n",
        "            conv_channels,\n",
        "            kernel_size,\n",
        "            groups=conv_channels,\n",
        "            padding=dconv_pad,\n",
        "            dilation=dilation,\n",
        "            bias=True)\n",
        "        self.prelu2 = nn.PReLU()\n",
        "        self.lnorm2 = build_norm(norm, conv_channels)\n",
        "        # 1x1 conv cross channel\n",
        "        self.sconv = nn.Conv1d(conv_channels, in_channels, 1, bias=True)\n",
        "        # different padding way\n",
        "        self.causal = causal\n",
        "        self.dconv_pad = dconv_pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1x1(x)\n",
        "        y = self.lnorm1(self.prelu1(y))\n",
        "        y = self.dconv(y)\n",
        "        if self.causal:\n",
        "            y = y[:, :, :-self.dconv_pad]\n",
        "        y = self.lnorm2(self.prelu2(y))\n",
        "        y = self.sconv(y)\n",
        "        x = x + y\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTasNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 L=20,\n",
        "                 N=256,\n",
        "                 X=8,\n",
        "                 R=4,\n",
        "                 B=256,\n",
        "                 H=512,\n",
        "                 P=3,\n",
        "                 norm=\"cLN\",\n",
        "                 num_spks=2,\n",
        "                 non_linear=\"relu\",\n",
        "                 causal=False):\n",
        "        super(ConvTasNet, self).__init__()\n",
        "        supported_nonlinear = {\n",
        "            \"relu\": F.relu,\n",
        "            \"sigmoid\": th.sigmoid,\n",
        "            \"softmax\": F.softmax\n",
        "        }\n",
        "        if non_linear not in supported_nonlinear:\n",
        "            raise RuntimeError(\"Unsupported non-linear function: {}\",\n",
        "                               format(non_linear))\n",
        "        self.non_linear_type = non_linear\n",
        "        self.non_linear = supported_nonlinear[non_linear]\n",
        "        # n x S => n x N x T, S = 4s*8000 = 32000\n",
        "        self.encoder_1d = Conv1D(1, N, L, stride=L // 2, padding=0)\n",
        "        # keep T not change\n",
        "        # T = int((xlen - L) / (L // 2)) + 1\n",
        "        # before repeat blocks, always cLN\n",
        "        self.ln = ChannelWiseLayerNorm(N)\n",
        "        # n x N x T => n x B x T\n",
        "        self.proj = Conv1D(N, B, 1)\n",
        "        # repeat blocks\n",
        "        # n x B x T => n x B x T\n",
        "        self.repeats = self._build_repeats(\n",
        "            R,\n",
        "            X,\n",
        "            in_channels=B,\n",
        "            conv_channels=H,\n",
        "            kernel_size=P,\n",
        "            norm=norm,\n",
        "            causal=causal)\n",
        "        # output 1x1 conv\n",
        "        # n x B x T => n x N x T\n",
        "        # NOTE: using ModuleList not python list\n",
        "        # self.conv1x1_2 = th.nn.ModuleList(\n",
        "        #     [Conv1D(B, N, 1) for _ in range(num_spks)])\n",
        "        # n x B x T => n x 2N x T\n",
        "        self.mask = Conv1D(B, num_spks * N, 1)\n",
        "        # using ConvTrans1D: n x N x T => n x 1 x To\n",
        "        # To = (T - 1) * L // 2 + L\n",
        "        self.decoder_1d = ConvTrans1D(\n",
        "            N, 1, kernel_size=L, stride=L // 2, bias=True)\n",
        "        self.num_spks = num_spks\n",
        "\n",
        "    def _build_blocks(self, num_blocks, **block_kwargs):\n",
        "        \"\"\"\n",
        "        Build Conv1D block\n",
        "        \"\"\"\n",
        "        blocks = [\n",
        "            Conv1DBlock(**block_kwargs, dilation=(2**b))\n",
        "            for b in range(num_blocks)\n",
        "        ]\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def _build_repeats(self, num_repeats, num_blocks, **block_kwargs):\n",
        "        \"\"\"\n",
        "        Build Conv1D block repeats\n",
        "        \"\"\"\n",
        "        repeats = [\n",
        "            self._build_blocks(num_blocks, **block_kwargs)\n",
        "            for r in range(num_repeats)\n",
        "        ]\n",
        "        return nn.Sequential(*repeats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() >= 3:\n",
        "            raise RuntimeError(\n",
        "                \"{} accept 1/2D tensor as input, but got {:d}\".format(\n",
        "                    self.__name__, x.dim()))\n",
        "        # when inference, only one utt\n",
        "        if x.dim() == 1:\n",
        "            x = th.unsqueeze(x, 0)\n",
        "        # n x 1 x S => n x N x T\n",
        "        w = F.relu(self.encoder_1d(x))\n",
        "        # n x B x T\n",
        "        y = self.proj(self.ln(w))\n",
        "        # n x B x T\n",
        "        y = self.repeats(y)\n",
        "        # n x 2N x T\n",
        "        e = th.chunk(self.mask(y), self.num_spks, 1)\n",
        "        # n x N x T\n",
        "        if self.non_linear_type == \"softmax\":\n",
        "            m = self.non_linear(th.stack(e, dim=0), dim=0)\n",
        "        else:\n",
        "            m = self.non_linear(th.stack(e, dim=0))\n",
        "        # spks x [n x N x T]\n",
        "        s = [w * m[n] for n in range(self.num_spks)]\n",
        "        # spks x n x S\n",
        "        return [self.decoder_1d(x, squeeze=True) for x in s]\n",
        "\n",
        "def foo_conv_tas_net():\n",
        "    x = th.rand(4, 1000)\n",
        "    nnet = ConvTasNet(norm=\"cLN\", causal=False)\n",
        "    # print(nnet)\n",
        "    print(\"ConvTasNet #param: {:.2f}\".format(param(nnet)))\n",
        "    x = nnet(x)\n",
        "    s1 = x[0]\n",
        "    print(s1.shape)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    foo_conv_tas_net()\n",
        "    # foo_conv1d_block()\n",
        "    # foo_layernorm()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvTasNet #param: 8.75\n",
            "torch.Size([4, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AesLHW2tVPul"
      },
      "source": [
        "#define dataloader\n",
        "import random\n",
        "import torch as th\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import torch.utils.data as dat\n",
        "\n",
        "# from .audio import WaveReader\n",
        "\n",
        "\n",
        "def make_dataloader(train=True,\n",
        "                    data_kwargs=None,\n",
        "                    num_workers=4,\n",
        "                    chunk_size=32000,\n",
        "                    batch_size=16):\n",
        "    dataset = Dataset(**data_kwargs)\n",
        "    return DataLoader(dataset,\n",
        "                      train=train,\n",
        "                      chunk_size=chunk_size,\n",
        "                      batch_size=batch_size,\n",
        "                      num_workers=num_workers)\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    \"\"\"\n",
        "    Per Utterance Loader\n",
        "    \"\"\"\n",
        "    def __init__(self, mix_scp=\"\", ref_scp=None, sample_rate=8000):\n",
        "        self.mix = WaveReader(mix_scp, sample_rate=sample_rate)\n",
        "        self.ref = [\n",
        "            WaveReader(ref, sample_rate=sample_rate) for ref in ref_scp\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mix)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        key = self.mix.index_keys[index]\n",
        "        mix = self.mix[key]\n",
        "        ref = [reader[key] for reader in self.ref]\n",
        "        return {\n",
        "            \"mix\": mix.astype(np.float32),\n",
        "            \"ref\": [r.astype(np.float32) for r in ref]\n",
        "        }\n",
        "\n",
        "\n",
        "class ChunkSplitter(object):\n",
        "    \"\"\"\n",
        "    Split utterance into small chunks\n",
        "    \"\"\"\n",
        "    def __init__(self, chunk_size, train=True, least=16000):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.least = least\n",
        "        self.train = train\n",
        "\n",
        "    def _make_chunk(self, eg, s):\n",
        "        \"\"\"\n",
        "        Make a chunk instance, which contains:\n",
        "            \"mix\": ndarray,\n",
        "            \"ref\": [ndarray...]\n",
        "        \"\"\"\n",
        "        chunk = dict()\n",
        "        chunk[\"mix\"] = eg[\"mix\"][s:s + self.chunk_size]\n",
        "        chunk[\"ref\"] = [ref[s:s + self.chunk_size] for ref in eg[\"ref\"]]\n",
        "        return chunk\n",
        "\n",
        "    def split(self, eg):\n",
        "        N = eg[\"mix\"].size\n",
        "        # too short, throw away\n",
        "        if N < self.least:\n",
        "            return []\n",
        "        chunks = []\n",
        "        # padding zeros\n",
        "        if N < self.chunk_size:\n",
        "            P = self.chunk_size - N\n",
        "            chunk = dict()\n",
        "            chunk[\"mix\"] = np.pad(eg[\"mix\"], (0, P), \"constant\")\n",
        "            chunk[\"ref\"] = [\n",
        "                np.pad(ref, (0, P), \"constant\") for ref in eg[\"ref\"]\n",
        "            ]\n",
        "            chunks.append(chunk)\n",
        "        else:\n",
        "            # random select start point for training\n",
        "            s = random.randint(0, N % self.least) if self.train else 0\n",
        "            while True:\n",
        "                if s + self.chunk_size > N:\n",
        "                    break\n",
        "                chunk = self._make_chunk(eg, s)\n",
        "                chunks.append(chunk)\n",
        "                s += self.least\n",
        "        return chunks\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    \"\"\"\n",
        "    Online dataloader for chunk-level PIT\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 num_workers=4,\n",
        "                 chunk_size=32000,\n",
        "                 batch_size=16,\n",
        "                 train=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.train = train\n",
        "        self.splitter = ChunkSplitter(chunk_size,\n",
        "                                      train=train,\n",
        "                                      least=chunk_size // 2)\n",
        "        # just return batch of egs, support multiple workers\n",
        "        self.eg_loader = dat.DataLoader(dataset,\n",
        "                                        batch_size=batch_size // 2,\n",
        "                                        num_workers=num_workers,\n",
        "                                        shuffle=train,\n",
        "                                        collate_fn=self._collate)\n",
        "\n",
        "    def _collate(self, batch):\n",
        "        \"\"\"\n",
        "        Online split utterances\n",
        "        \"\"\"\n",
        "        chunk = []\n",
        "        for eg in batch:\n",
        "            chunk += self.splitter.split(eg)\n",
        "        return chunk\n",
        "\n",
        "    def _merge(self, chunk_list):\n",
        "        \"\"\"\n",
        "        Merge chunk list into mini-batch\n",
        "        \"\"\"\n",
        "        N = len(chunk_list)\n",
        "        if self.train:\n",
        "            random.shuffle(chunk_list)\n",
        "        blist = []\n",
        "        for s in range(0, N - self.batch_size + 1, self.batch_size):\n",
        "            batch = default_collate(chunk_list[s:s + self.batch_size])\n",
        "            blist.append(batch)\n",
        "        rn = N % self.batch_size\n",
        "        return blist, chunk_list[-rn:] if rn else []\n",
        "\n",
        "    def __iter__(self):\n",
        "        chunk_list = []\n",
        "        for chunks in self.eg_loader:\n",
        "            chunk_list += chunks\n",
        "            batch, chunk_list = self._merge(chunk_list)\n",
        "            for obj in batch:\n",
        "                yield obj\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0b6I4FKMu0X"
      },
      "source": [
        "#define train process\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from itertools import permutations\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch as th\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import json\n",
        "import logging\n",
        "\n",
        "\n",
        "def get_logger(\n",
        "        name,\n",
        "        format_str=\"%(asctime)s [%(pathname)s:%(lineno)s - %(levelname)s ] %(message)s\",\n",
        "        date_format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        file=False):\n",
        "    \"\"\"\n",
        "    Get python logger instance\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    # file or console\n",
        "    handler = logging.StreamHandler() if not file else logging.FileHandler(\n",
        "        name)\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter(fmt=format_str, datefmt=date_format)\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    return logger\n",
        "\n",
        "\n",
        "def dump_json(obj, fdir, name):\n",
        "    \"\"\"\n",
        "    Dump python object in json\n",
        "    \"\"\"\n",
        "    if fdir and not os.path.exists(fdir):\n",
        "        os.makedirs(fdir)\n",
        "    with open(os.path.join(fdir, name), \"w\") as f:\n",
        "        json.dump(obj, f, indent=4, sort_keys=False)\n",
        "\n",
        "\n",
        "def load_json(fdir, name):\n",
        "    \"\"\"\n",
        "    Load json as python object\n",
        "    \"\"\"\n",
        "    path = os.path.join(fdir, name)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"Could not find json file: {}\".format(path))\n",
        "    with open(path, \"r\") as f:\n",
        "        obj = json.load(f)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def load_obj(obj, device):\n",
        "    \"\"\"\n",
        "    Offload tensor object in obj to cuda device\n",
        "    \"\"\"\n",
        "\n",
        "    def cuda(obj):\n",
        "        return obj.to(device) if isinstance(obj, th.Tensor) else obj\n",
        "\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: load_obj(obj[key], device) for key in obj}\n",
        "    elif isinstance(obj, list):\n",
        "        return [load_obj(val, device) for val in obj]\n",
        "    else:\n",
        "        return cuda(obj)\n",
        "\n",
        "\n",
        "class SimpleTimer(object):\n",
        "    \"\"\"\n",
        "    A simple timer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.start = time.time()\n",
        "\n",
        "    def elapsed(self):\n",
        "        return (time.time() - self.start) / 60\n",
        "\n",
        "\n",
        "class ProgressReporter(object):\n",
        "    \"\"\"\n",
        "    A simple progress reporter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger, period=100):\n",
        "        self.period = period\n",
        "        self.logger = logger\n",
        "        self.loss = []\n",
        "        self.timer = SimpleTimer()\n",
        "\n",
        "    def add(self, loss):\n",
        "        self.loss.append(loss)\n",
        "        N = len(self.loss)\n",
        "        if not N % self.period:\n",
        "            avg = sum(self.loss[-self.period:]) / self.period\n",
        "            self.logger.info(\"Processed {:d} batches\"\n",
        "                             \"(loss = {:+.2f})...\".format(N, avg))\n",
        "\n",
        "    def report(self, details=False):\n",
        "        N = len(self.loss)\n",
        "        if details:\n",
        "            sstr = \",\".join(map(lambda f: \"{:.2f}\".format(f), self.loss))\n",
        "            self.logger.info(\"Loss on {:d} batches: {}\".format(N, sstr))\n",
        "        return {\n",
        "            \"loss\": sum(self.loss) / N,\n",
        "            \"batches\": N,\n",
        "            \"cost\": self.timer.elapsed()\n",
        "        }\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self,\n",
        "                 nnet,\n",
        "                 checkpoint=\"checkpoint\",\n",
        "                 optimizer=\"adam\",\n",
        "                 gpuid=0,\n",
        "                 optimizer_kwargs=None,\n",
        "                 clip_norm=None,\n",
        "                 min_lr=0,\n",
        "                 patience=0,\n",
        "                 factor=0.5,\n",
        "                 logging_period=100,\n",
        "                 resume=None,\n",
        "                 no_impr=6):\n",
        "        if not th.cuda.is_available():\n",
        "            raise RuntimeError(\"CUDA device unavailable...exist\")\n",
        "        if not isinstance(gpuid, tuple):\n",
        "            gpuid = (gpuid, )\n",
        "        self.device = th.device(\"cuda:{}\".format(gpuid[0]))\n",
        "        self.gpuid = gpuid\n",
        "        if checkpoint and not os.path.exists(checkpoint):\n",
        "            os.makedirs(checkpoint)\n",
        "        self.checkpoint = checkpoint\n",
        "        self.logger = get_logger(\n",
        "            os.path.join(checkpoint, \"trainer.log\"), file=True)\n",
        "\n",
        "        self.clip_norm = clip_norm\n",
        "        self.logging_period = logging_period\n",
        "        self.cur_epoch = 0  # zero based\n",
        "        self.no_impr = no_impr\n",
        "\n",
        "        if resume:\n",
        "            if not os.path.exists(resume):\n",
        "                raise FileNotFoundError(\n",
        "                    \"Could not find resume checkpoint: {}\".format(resume))\n",
        "            cpt = th.load(resume, map_location=\"cpu\")\n",
        "            self.cur_epoch = cpt[\"epoch\"]\n",
        "            self.logger.info(\"Resume from checkpoint {}: epoch {:d}\".format(\n",
        "                resume, self.cur_epoch))\n",
        "            # load nnet\n",
        "            nnet.load_state_dict(cpt[\"model_state_dict\"])\n",
        "            self.nnet = nnet.to(self.device)\n",
        "            self.optimizer = self.create_optimizer(\n",
        "                optimizer, optimizer_kwargs, state=cpt[\"optim_state_dict\"])\n",
        "        else:\n",
        "            self.nnet = nnet.to(self.device)\n",
        "            self.optimizer = self.create_optimizer(optimizer, optimizer_kwargs)\n",
        "        self.scheduler = ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode=\"min\",\n",
        "            factor=factor,\n",
        "            patience=patience,\n",
        "            min_lr=min_lr,\n",
        "            verbose=True)\n",
        "        self.num_params = sum(\n",
        "            [param.nelement() for param in nnet.parameters()]) / 10.0**6\n",
        "\n",
        "        # logging\n",
        "        self.logger.info(\"Model summary:\\n{}\".format(nnet))\n",
        "        self.logger.info(\"Loading model to GPUs:{}, #param: {:.2f}M\".format(\n",
        "            gpuid, self.num_params))\n",
        "        if clip_norm:\n",
        "            self.logger.info(\n",
        "                \"Gradient clipping by {}, default L2\".format(clip_norm))\n",
        "\n",
        "    def save_checkpoint(self, best=True):\n",
        "        cpt = {\n",
        "            \"epoch\": self.cur_epoch,\n",
        "            \"model_state_dict\": self.nnet.state_dict(),\n",
        "            \"optim_state_dict\": self.optimizer.state_dict()\n",
        "        }\n",
        "        th.save(\n",
        "            cpt,\n",
        "            os.path.join(self.checkpoint,\n",
        "                         \"{0}.pt.tar\".format(\"best\" if best else \"last\")))\n",
        "\n",
        "    def create_optimizer(self, optimizer, kwargs, state=None):\n",
        "        supported_optimizer = {\n",
        "            \"sgd\": th.optim.SGD,  # momentum, weight_decay, lr\n",
        "            \"rmsprop\": th.optim.RMSprop,  # momentum, weight_decay, lr\n",
        "            \"adam\": th.optim.Adam,  # weight_decay, lr\n",
        "            \"adadelta\": th.optim.Adadelta,  # weight_decay, lr\n",
        "            \"adagrad\": th.optim.Adagrad,  # lr, lr_decay, weight_decay\n",
        "            \"adamax\": th.optim.Adamax  # lr, weight_decay\n",
        "            # ...\n",
        "        }\n",
        "        if optimizer not in supported_optimizer:\n",
        "            raise ValueError(\"Now only support optimizer {}\".format(optimizer))\n",
        "        opt = supported_optimizer[optimizer](self.nnet.parameters(), **kwargs)\n",
        "        self.logger.info(\"Create optimizer {0}: {1}\".format(optimizer, kwargs))\n",
        "        if state is not None:\n",
        "            opt.load_state_dict(state)\n",
        "            self.logger.info(\"Load optimizer state dict from checkpoint\")\n",
        "        return opt\n",
        "\n",
        "    def compute_loss(self, egs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train(self, data_loader):\n",
        "        self.logger.info(\"Set train mode...\")\n",
        "        self.nnet.train()\n",
        "        reporter = ProgressReporter(self.logger, period=self.logging_period)\n",
        "\n",
        "        for egs in data_loader:\n",
        "            # load to gpu\n",
        "            egs = load_obj(egs, self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.compute_loss(egs)\n",
        "            loss.backward()\n",
        "            if self.clip_norm:\n",
        "                clip_grad_norm_(self.nnet.parameters(), self.clip_norm)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            reporter.add(loss.item())\n",
        "        return reporter.report()\n",
        "\n",
        "    def eval(self, data_loader):\n",
        "        self.logger.info(\"Set eval mode...\")\n",
        "        self.nnet.eval()\n",
        "        reporter = ProgressReporter(self.logger, period=self.logging_period)\n",
        "\n",
        "        with th.no_grad():\n",
        "            for egs in data_loader:\n",
        "                egs = load_obj(egs, self.device)\n",
        "                loss = self.compute_loss(egs)\n",
        "                reporter.add(loss.item())\n",
        "        return reporter.report(details=True)\n",
        "\n",
        "    def run(self, train_loader, dev_loader, num_epochs=50):\n",
        "        # avoid alloc memory from gpu0\n",
        "        with th.cuda.device(self.gpuid[0]):\n",
        "            stats = dict()\n",
        "            # check if save is OK\n",
        "            self.save_checkpoint(best=False)\n",
        "            cv = self.eval(dev_loader)\n",
        "            best_loss = cv[\"loss\"]\n",
        "            self.logger.info(\"START FROM EPOCH {:d}, LOSS = {:.4f}\".format(\n",
        "                self.cur_epoch, best_loss))\n",
        "            no_impr = 0\n",
        "            # make sure not inf\n",
        "            self.scheduler.best = best_loss\n",
        "            while self.cur_epoch < num_epochs:\n",
        "                self.cur_epoch += 1\n",
        "                cur_lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "                stats[\n",
        "                    \"title\"] = \"Loss(time/N, lr={:.3e}) - Epoch {:2d}:\".format(\n",
        "                        cur_lr, self.cur_epoch)\n",
        "                tr = self.train(train_loader)\n",
        "                stats[\"tr\"] = \"train = {:+.4f}({:.2f}m/{:d})\".format(\n",
        "                    tr[\"loss\"], tr[\"cost\"], tr[\"batches\"])\n",
        "                cv = self.eval(dev_loader)\n",
        "                stats[\"cv\"] = \"dev = {:+.4f}({:.2f}m/{:d})\".format(\n",
        "                    cv[\"loss\"], cv[\"cost\"], cv[\"batches\"])\n",
        "                stats[\"scheduler\"] = \"\"\n",
        "                if cv[\"loss\"] > best_loss:\n",
        "                    no_impr += 1\n",
        "                    stats[\"scheduler\"] = \"| no impr, best = {:.4f}\".format(\n",
        "                        self.scheduler.best)\n",
        "                else:\n",
        "                    best_loss = cv[\"loss\"]\n",
        "                    no_impr = 0\n",
        "                    self.save_checkpoint(best=True)\n",
        "                self.logger.info(\n",
        "                    \"{title} {tr} | {cv} {scheduler}\".format(**stats))\n",
        "                # schedule here\n",
        "                self.scheduler.step(cv[\"loss\"])\n",
        "                # flush scheduler info\n",
        "                sys.stdout.flush()\n",
        "                # save last checkpoint\n",
        "                self.save_checkpoint(best=False)\n",
        "                if no_impr == self.no_impr:\n",
        "                    self.logger.info(\n",
        "                        \"Stop training cause no impr for {:d} epochs\".format(\n",
        "                            no_impr))\n",
        "                    break\n",
        "            self.logger.info(\"Training for {:d}/{:d} epoches done!\".format(\n",
        "                self.cur_epoch, num_epochs))\n",
        "\n",
        "\n",
        "class SiSnrTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(SiSnrTrainer, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def sisnr(self, x, s, eps=1e-8):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        x: separated signal, N x S tensor\n",
        "        s: reference signal, N x S tensor\n",
        "        Return:\n",
        "        sisnr: N tensor\n",
        "        \"\"\"\n",
        "\n",
        "        def l2norm(mat, keepdim=False):\n",
        "            return th.norm(mat, dim=-1, keepdim=keepdim)\n",
        "\n",
        "        if x.shape != s.shape:\n",
        "            raise RuntimeError(\n",
        "                \"Dimention mismatch when calculate si-snr, {} vs {}\".format(\n",
        "                    x.shape, s.shape))\n",
        "        x_zm = x - th.mean(x, dim=-1, keepdim=True)\n",
        "        s_zm = s - th.mean(s, dim=-1, keepdim=True)\n",
        "        t = th.sum(\n",
        "            x_zm * s_zm, dim=-1,\n",
        "            keepdim=True) * s_zm / (l2norm(s_zm, keepdim=True)**2 + eps)\n",
        "        return 20 * th.log10(eps + l2norm(t) / (l2norm(x_zm - t) + eps))\n",
        "\n",
        "    def compute_loss(self, egs):\n",
        "        # spks x n x S\n",
        "        ests = th.nn.parallel.data_parallel(\n",
        "            self.nnet, egs[\"mix\"], device_ids=self.gpuid)\n",
        "        # spks x n x S\n",
        "        refs = egs[\"ref\"]\n",
        "        num_spks = len(refs)\n",
        "\n",
        "        def sisnr_loss(permute):\n",
        "            # for one permute\n",
        "            return sum(\n",
        "                [self.sisnr(ests[s], refs[t])\n",
        "                 for s, t in enumerate(permute)]) / len(permute)\n",
        "\n",
        "        # P x N\n",
        "        N = egs[\"mix\"].size(0)\n",
        "        sisnr_mat = th.stack(\n",
        "            [sisnr_loss(p) for p in permutations(range(num_spks))])\n",
        "        max_perutt, _ = th.max(sisnr_mat, dim=0)\n",
        "        # si-snr\n",
        "        return -th.sum(max_perutt) / N\n",
        "\n",
        "def run(args):\n",
        "    gpuids = tuple(map(int, args.gpus.split(\",\")))\n",
        "\n",
        "    nnet = ConvTasNet(**nnet_conf)\n",
        "    trainer = SiSnrTrainer(nnet,\n",
        "                           gpuid=gpuids,\n",
        "                           checkpoint=args.checkpoint,\n",
        "                           resume=args.resume,\n",
        "                           **trainer_conf)\n",
        "\n",
        "    data_conf = {\n",
        "        \"train\": train_data,\n",
        "        \"dev\": dev_data,\n",
        "        \"chunk_size\": chunk_size\n",
        "    }\n",
        "    for conf, fname in zip([nnet_conf, trainer_conf, data_conf],\n",
        "                           [\"mdl.json\", \"trainer.json\", \"data.json\"]):\n",
        "        dump_json(conf, args.checkpoint, fname)\n",
        "\n",
        "    train_loader = make_dataloader(train=True,\n",
        "                                   data_kwargs=train_data,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   chunk_size=chunk_size,\n",
        "                                   num_workers=args.num_workers)\n",
        "    dev_loader = make_dataloader(train=False,\n",
        "                                 data_kwargs=dev_data,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 chunk_size=chunk_size,\n",
        "                                 num_workers=args.num_workers)\n",
        "\n",
        "    trainer.run(train_loader, dev_loader, num_epochs=args.epochs)"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}